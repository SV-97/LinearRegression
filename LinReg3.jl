import Random
Random.seed!(0)

using Plots
using LinearAlgebra

# clear all files
open("norm_gradient_w.txt", "w") do io
end

open("error.txt", "w") do io
end

open("learning_rate.txt", "w") do io
end

"Sum from k=`from` to `to` of `a(k)`"
Î£(from::Integer, to::Integer, a::Function, zero = 0) = mapreduce(a, (+), from:to; init = zero)

"""Linear Regression
# Args:
    ğ°: Parameters
    Î¦(j, ğ±): Basis function of type (Int, Vector{T}) -> T
    ğ±: Input vector
"""
function y(ğ°::Vector{<:Number},
  Î¦::(T where T <: Function),
  ğ±::Vector{<:Number})::Number
    Î£(1, size(ğ°)[1], j->ğ°[j] * Î¦(j, ğ±))
end

"""Derivative of E_D with respect to ğ°â‚–
# Args:
    Î¦(k, ğ±â‚™): Basis function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    k: Index for ğ°â‚– in respect to which the derivative is taken
    ğ°: Parameters
"""
function âˆ‚E_Dâˆ‚w_k(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number},
  k::Integer)::Number
    N = size(t)[1]
    - Î£(1, N, n->Î¦(k, ğ—[n, :]) * (t[n] - y(ğ°, Î¦, ğ—[n,:])))
end

"""Error function
# Args:
    Î¦(k, ğ±â‚™): Basis function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    ğ°: Parameters
"""
function E_D(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number})::Number
    N = size(t)[1]
    1 // 2 * Î£(1, N, n->(t[n] - y(ğ°, Î¦, ğ—[n,:]))^2)
end

"""One iteration of the gradient descent algorithm
# Args:
    âˆ‚E_Dâˆ‚w_k: Partial derivative of error function with respect to
        the k-th parameter
    ğ—: Column vector of inputs ğ±â‚™ where ğ±â‚™ is an input vector to the
        error function
    t: corresponding target values for each ğ±â‚™
    ğ°: Parameters
    Î·: Learning rate
    âˆ‡ğ°_prior: Gradient of parameters from prior iteration
    Î³: Momentum factor
"""
function gradient_descent_iteration(âˆ‚E_Dâˆ‚w_k::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number},
  Î·::Number,
  âˆ‡ğ°_prior::Vector{<:Number},
  Î³::Number)::Tuple{Vector{<:Number},Vector{<:Number}}
    M = size(ğ°)[1]
    âˆ‡ğ° = Î³ * âˆ‡ğ°_prior
    for j = 1:M
        âˆ‚E_Dâˆ‚w_jk(k) = âˆ‚E_Dâˆ‚w_k(ğ—, t, ğ°, k)
        âˆ‡ğ° += collect(map(âˆ‚E_Dâˆ‚w_jk, 1:M))

        open("norm_gradient_w.txt", "a") do io
            write(io, string(norm(âˆ‡ğ°)), "\n")
        end
    end
    (ğ° - Î· * âˆ‡ğ°, âˆ‡ğ°)
end

"""Minimize function E_D(ğ—, t, ğ°)
# Args:
    âˆ‚E_Dâˆ‚w_k: Partial derivative of error function with respect to
        the k-th parameter
    ğ—: Column vector of inputs ğ±â‚™ where ğ±â‚™ is an input vector to the
        error function
    t: corresponding target values for each ğ±â‚™
    ğ°: Initial parameters - usually `randn(M)`
    Î·: Learning rate
    M: Number of model parameters
    iters: Number of iterations
    Îµ: Gradient descent stops once the difference between two iterations
        (ğ° and ğ°') is less than Îµ
    Î³: Momentum factor

# Fails:
    Fails on encountering NaN in computation or on Divergence to Inf
"""
function gradient_descent(âˆ‚E_Dâˆ‚w_k::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  Î·::Number,
  M::Integer,
  iters::Integer,
  ğ°::Vector{<:Number},
  Îµ = 10e-12::Number,
  Î³ = 0.9::Real)::Tuple{Vector{<:Number},Integer}
    âˆ‡ğ° = zero(ğ°)
    did_iters = 0
    for i = 1:iters
        did_iters += 1
        ğ°_old = ğ°
        open("error.txt", "a") do io
            write(io, string(E_D(Î¦, ğ—, t, ğ°)), "\n")
        end
        open("learning_rate.txt", "a") do io
            write(io, string(Î·), "\n")
        end
        (ğ°, âˆ‡ğ°) = gradient_descent_iteration(âˆ‚E_Dâˆ‚w_k, ğ—, t, ğ°, Î·, âˆ‡ğ°, Î³)
         if any(isnan.(ğ°))
           error("Encountered NaN") 
        end
        if any(isinf.(ğ°))
            error("Divergence in calculation")
        end
        if norm(ğ°_old - ğ°) < Îµ
            break
        end
    end
    (ğ°, did_iters)
end

"""Find regression model 
# Args:
    Î¦: Basis Function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    Î·: learning rate with which to train
    M: Number of model parameters
    iters: Number of iterations
    Îµ: Gradient descent stops once the difference between
        two iterations (ğ° and ğ°') is less than Îµ
    Î³: Momentum Parameter
    optimizer: Parameter to select optimizer that's used

# Fails:
    On unknown optimizers or error inside the optimizer
"""
function fit_linear_model(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  Î·::Number,
  M::Integer,
  iters::Integer,
  Îµ = 10e-12::Number,
  Î³ = 0.9::Real,
  optimizer = :gradient_descent)::Tuple{Function,Number}
    if optimizer == :gradient_descent
        (ğ°, did_iters) = gradient_descent((ğ—, t, ğ°, k)->âˆ‚E_Dâˆ‚w_k(Î¦, ğ—, t, ğ°, k),
            ğ—, t, Î·, M, iters, randn(M), Îµ, Î³)
        residual_error = E_D(Î¦, ğ—, t, ğ°)
        println(ğ°, " after ", did_iters, " iterations. Residual error: ", residual_error)
        (ğ±->y(ğ°, Î¦, ğ±), residual_error)
    else
        error("Invalid optimizer")
    end
end

# optimal solution using a linear model for comparison

x_b = [1, 2, 3]
y_b = [1, 1, 2]

function fitline(x::Vector{<:Number}, y::Vector{<:Number})::Function
    n = size(x)[1]
    a = (sum(y) * sum(x.^2) - sum(x) * sum(x .* y)) / (n * sum(x.^2) - sum(x)^2) # t
    b = (n * sum(x .* y) - sum(x) * sum(y)) / (n * sum(x.^2) - sum(x)^2)# m
    x->b * x + a
end

optimal_linear_model = fitline(x_b, y_b)

# end of optimal solution

# a few different basis functions for testing
Î¦1(j, ğ±) =
    if j == 0
        1
    else
        ğ±[1]
end

Î¦2(j, ğ±) = ğ±[1]^j
Î¦3(j, ğ±) = sin(1 / j * ğ±[1])
Ïƒ(a) = 1 / (1 + exp(-a))
function Î¦4(j, ğ±)
    Î¼ = 0.2
    s = 0.2
    Ïƒ((ğ±[1] - Î¼) / s)
end

# test3
ğ— = hcat([0; 1; 2; 3; 4; 5]) # hcat to convert to matrix because julia is weird like that
t = [0, 1, 4, 9, 16, 25]
t += randn(size(t)[1]) * 3


Î¦ = Î¦2

(model1, residual_error) = fit_linear_model(Î¦2, ğ—, t, 0.00005, 2, 300000, 10e-3, 0.5) # Î¦2, ğ—, t, 0.00005, 2, 300000, 10e-5, 0.2)


x = 0:0.1:5

p = scatter(map(x->x[1], ğ—), t, label = "training");
plot!(x, model1.([[x] for x in x]), label = "prediction")
display(p)
readline()
