\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[german]{babel} % prefer english over german
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{angles,arrows,babel,calc,patterns,quotes}

\usepackage{graphicx}
\graphicspath{ {./img/} }

\usepackage{minted}
\usepackage{xcolor}

\usepackage{eurosym}
\usepackage{amstext}

\usepackage{booktabs}

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\usepackage{fontspec}

\setmonofont[Mapping=tex-text, Scale=0.90,]{Fira Code}
%\setmonofont[Mapping=tex-text, Scale=0.90,]{Droid Sans Mono}

\theoremstyle{plain} %Text ist Kursiv
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Korollar}

\theoremstyle{definition} %Text ist \"upright"
\newtheorem{remark}[theorem]{Bemerkung}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Beispiel}
\newtheorem{algo}[theorem]{Algorithm}
\newtheorem{problem}[theorem]{Problem}
\let\proof\undefined
\newtheorem{proof}[theorem]{Beweis}
\newtheorem{theo}[theorem]{Satz}
\newtheorem{anno}[theorem]{Anmerkung}
\newtheorem{solution}[theorem]{LÃ¶sung}

% Colors
\definecolor{bg}{rgb}{0.95,0.95,0.95}

\newcommand{\sfloor}[1]{\left\lfloor #1 \right\rfloor} % scaling floor function
\newcommand{\sceil}[1]{\left\lceil #1 \right\rceil} % scaling ceil function
\newcommand{\floor}[1]{\lfloor #1 \rfloor} % floor function
\newcommand{\ceil}[1]{\lceil #1 \rceil} % ceil function
\newcommand{\specialset}[1]{\rm I\!#1} % render sets like R and N fancy
\newcommand{\abs}[1]{\left\lVert#1\right\rVert} % Absolute value of #1
\newcommand{\unit}[1]{\hat{#1}} % unit vector

\title{ N-dimensionales Datenfitting mit linearer Regression  }


\author{
  Stefan Volz
}

\begin{document}

\begin{abstract}
  In vielen Anwendungen ist es nÃ¶tig/vorteilhaft aus Messdaten ein Mathematisches Modell zu entwickeln, welches mÃ¶glichst Messungenauigkeiten ausgleicht bzw. Vorhersagen zulÃ¤sst. Ziel der Arbeit ist die Implementierung von linearer Regression als Funktion hÃ¶herer Ordnung, welche mittels Gradientenabstieg eine Fehlerminimierung des Modells im Bezug auf gegebene Messdaten durchfÃ¼hrt. Die Implementierung erfolgt in Julia.
\end{abstract}

\maketitle
\section{Konventionen und Generelles}
\subsection{Wahl der Programmiersprache und AbhÃ¤ngigkeiten}
Die Implementierung erfolgt in Julia\footnote{\url{https://julialang.org/}}. Julia ist eine hochperformante, stark und dynamisch typisierte, Ã¼berwiegend imperative Programmiersprache mit Fokus auf wissenschaftlichem Rechnen; ist jedoch im Gegensatz zu z.B. MATLAB als General Purpose Sprache zu verstehen. Die Arbeit benutzt Julia in Version 1.2.0. Die AbhÃ¤ngigkeiten beschrÃ¤nken sich auf das \texttt{Plots} Package\footnote{\url{https://docs.juliaplots.org/}}.

Die Wahl der Sprache fiel auf Julia, da es nativ eine sehr gute UnterstÃ¼tzung fÃ¼r Matrizen mitbringt was fÃ¼r die Implementierung als sehr vorteilhaft angesehen wurde. AuÃŸerdem erlaubt der gute Unicode-Support es, Identifier so zu wÃ¤hlen, dass diese nah an der Fachliteratur sind.

\subsection{Quellcode}
Beim Code wurde darauf geachtet, die Typannotationen\footnote{\url{https://docs.julialang.org/en/v1/manual/types/}} von Julia zu nutzen, da diese zum einfacheren VerstÃ¤ndnis des Codes beitragen und auÃŸerdem der Performance zutrÃ¤glich sind.
Identifier wurden grÃ¶ÃŸtenteils so gewÃ¤hlt, dass sie sich mit \cite{Bishop} decken.

\subsection{Text dieser Arbeit}
Im Text der Arbeit werden - in Anlehnung an \cite{Bishop} - folgende Konventionen genutzt:

\begin{tabular}{llc}
  \toprule
  Typ & Beschreibung & Beispiel \\
  \midrule
  vektorielle GrÃ¶ÃŸen & fettgedruckte Kleinbuchstaben & $\mathbf{w}$ \\
  einzelne Elemente eines Vektors & indizierte Kleinbuchstaben & $w_j$ \\
  Matrizen und Mengen & GroÃŸbuchstaben & $A$ \\
  Hyperparameter des Modells & griechische Kleinbuchstaben & $\gamma$ \\
  sonstige Parameter & lateinische Kleinbuchstaben & $x$ \\
  Code listings und Quellcode-Referenzen & monospace font & \texttt{code} \\
\end{tabular}

Einzelne Indizes an Matrixen wie z.B. $A_j$ sind als Zeilenindizes zu verstehen, sodass $A_j$ die j-te Zeile von $A$ ist.

\section{Grundlegende Implementierung}
\subsection{Problemformulierung}
Seien $d,k,M,N \in \mathbb{N}$ mit Messdaten $X \in M^{d \times N}$ und zugehÃ¶rigen Zielwerten $T \in Y^{k \times N}$ gegeben. Gesucht werden die Parameter $\mathbf{w}$ eines Modells $y(\mathbf{w}, \mathbf{x})$ mit $y \in W^M \times X^d \rightarrow Y^{k}$ welche das Minimierungsproblem
$$
    \min_{\mathbf{w} \in W^M}\sum_{i=1}^N E(y(\mathbf{w}, \mathbf{x}_{i,.}), \mathbf{t}_{i,.}),
$$
im Bezug auf eine Fehlerfunktion $E: Y^k \times Y^k \rightarrow Z$ lÃ¶sen.

Wir betrachten zunÃ¤chst nur Probleme fÃ¼r die $k=1$ gilt.

\subsection{Lineare Regression}
\subsubsection{Mathematische Grundlagen}
Bei linearer Regression handelt es sich um eine Methode des Ã¼berwachten Lernens.
\begin{definition}
  Lineare Regressions Modelle sind Modelle, welche sich im Bezug auf ihre Modellparameter linear verhalten\cite[S. 137f]{Bishop}. Sie stellen Funktionen der Form $y: W^M \times X^D \rightarrow Y^T, (\mathbf{w}, \mathbf{x}) \mapsto y(\mathbf{w}, \mathbf{x})$ dar. Die Anzahl der Modellparameter ist gegeben durch $M \in \mathbb{N}$, die Anzahl an EingangsgrÃ¶ÃŸen durch $D \in \mathbb{N}$ und die der ZielgrÃ¶ÃŸen durch $T \in \mathbb{N}$.
\end{definition}
\begin{anno}
  Dies bedeutet nicht, dass sie auch zwingend linear im Bezug auf die Eingangsvariablen sein mÃ¼ssen.
\end{anno}

Das einfachste lineare Regressions Modell ist eine Linearkombination der Eingangsvariablen 
$$
  y(\mathbf{w}, \mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + \hdots + w_Dx_D,
$$
diese spiegeln jedoch oftmals nicht die zugrunde-liegende Verteilung der realen Messwerte wider, und limitieren ein Modell mit $D$ Eingangsvariablen auf $D+1$ Modellparameter. Daher werden Basisfunktionen eingefÃ¼hrt und Lineare Regressionsmodelle als Linearkombination dieser gebildet.
\begin{definition}
  Eine Funktion $\Phi_j: X^D \rightarrow Y^T, \mathbf{x} \mapsto \Phi_j(\mathbf{x})$ bezeichnen wir als Basisfunktion.
\end{definition}
\begin{anno}
  Im Code werden Basisfunktionen als Funktionen $\Phi: \mathbb{N} \times X^D \rightarrow Y^T, (j, \mathbf{x}) \mapsto \Phi(j, \mathbf{x})$ implementiert.
\end{anno}

Mit diesen Basisfunktionen ergibt sich als Modellgleichung
$$
  y(\mathbf{w}, \mathbf{x}) = w_0 + \sum_{j=1}^{M-1}w_j\Phi_j(\mathbf{x}),
$$
wobei der Parameter $w_0$ auch als Bias-Parameter bezeichnet wird, da er einen festen Offset der Daten ermÃ¶glicht. Im Code werden wir diesen Bias-Parameter als ``normalen'' Parameter betrachten und als kleinsten Index in $\mathbf{w}$ $1$ wÃ¤hlen. Ein EingangsvariablenunabhÃ¤ngiger Offset ist leicht durch eine angepasste Definition der Basisfunktion erreichbar:
\[ \Phi: (j, \mathbf{x}) \mapsto
 \left\{
  \begin{array}{ll}
    1,& j=1, \\
    \Phi_j(\mathbf{x}), & sonst. \\  
  \end{array}
\right. \]

Damit vereinfacht sich die Darstellung der Modellgleichung zu
\begin{align}
  y(\mathbf{w}, \mathbf{x}) = \sum_{j=1}^{M}w_j\Phi(j, \mathbf{x}), \label{y}
\end{align}

\subsubsection{Implementierung}
Auf Basis dieser Formulierungen kÃ¶nnen wir mit der Implementierung beginnen.
Hierzu definieren wir uns zunÃ¤chst eine Funktion \texttt{$\Sigma$} um alle im Code vorkommenden Summen zu abzudecken.

\begin{listing}[!ht]
    \begin{minted}[bgcolor=bg]{julia}
        "Sum from k=`from` to `to` of `a(k)`"
        Î£(from::Integer, to::Integer, a::Function, zero = 0) =
          mapreduce(a, (+), from:to; init = zero)
    \end{minted}
    \caption{Funktion \texttt{Î£}}
\end{listing}

Die Funktion ist hierbei eine Implementierung von $\sum_{k=\text{from}}^\text{to}a(k)$. Bei \texttt{mapreduce} handelt es sich um eine eingebaute Funktion welche (hier) erst \texttt{a} Ã¼ber eine Sequenz mappt und anschlieÃŸend diese Sequenz mittels \texttt{(+)} reduziert/faltet. Dabei ist \texttt{(+)} ist der eingebaute Additionsoperator. Der optionale Parameter \texttt{zero} erlaubt es die Funktion auch fÃ¼r nicht-skalare Summen(bzw. jegliche Typen fÃ¼r die eine Implementierung zu \texttt{+} existiert) zu nutzen.

\begin{listing}[!ht]
    \begin{minted}[bgcolor=bg]{Julia}
        """Linear Regression
        # Args:
            ğ°: Parameters
            Î¦(j, ğ±): Basis function of type (Int, Vector{T}) -> T
            ğ±: Input vector
        """
        function y(
          ğ°::Vector{<:Number},
          Î¦::(T where T <: Function),
          ğ±::Vector{<:Number})::(T where T <: Number)
            Î£(1, size(ğ°)[1], j->ğ°[j] * Î¦(j, ğ±))
        end
    \end{minted}
    \caption{Funktion \texttt{y}}
    \label{listing:y}
\end{listing}

Diese Implementierung der Funktion \texttt{y} passt 1:1 zur mathematischen Formulierung in Gleichung \ref{y}\footnote{Die in Listing \ref{listing:y} genutzte Schreibweise \mintinline{julia}|Vector{<:Number}| bedeutet \emph{Vektor eines Typen T, wobei T ein Subtyp des Abstrakten Typs Number ist}.}.

%Test Zitat \cite{Lippe}

\subsection{LSQ - GauÃŸ'sche Methode kleinster Quadrate}

Als nÃ¤chstes benÃ¶tigen wir eine MÃ¶glichkeit um den Fehler des Systems zu ermitteln - sodass wir diesen im nÃ¤chsten Schritt minimieren kÃ¶nnen. Hier kommt das LSQ Verfahren zum Einsatz (really?). Hierbei wird entsprechend \cite[S. 140f]{Bishop} eine Fehlerfunktion
\begin{align}
    E_D := \frac{1}{2}\sum_{n=1}^{N}(\mathbf{t}_n - y(\mathbf{w}, \mathbf{x}))^2
\end{align}
definiert.
Mit dieser Definition kÃ¶nnen wir nun unser Minimierungsproblem wie folgt definieren:
$$
    \min_{\mathbf{w} \in W^M} \frac{1}{2}\sum_{n=1}^{N}(\mathbf{t}_n - y(\mathbf{w}, X_n))^2
$$ 

aufstellen.

\subsection{Gradientenabstiegsverfahren}

\section{Fortgeschrittene Features}
\subsection{Momentum Verfahren}
\subsection{Adaptive Lernrate}

\section{Beispiele}

\bibliographystyle{alphadin}
\bibliography{Quellen}

\end{document}