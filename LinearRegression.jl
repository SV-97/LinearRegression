module LinearRegression

import Random

using LinearAlgebra

export fit_linear_model

# clear all files
open("norm_gradient_w.txt", "w") do io
end

open("error.txt", "w") do io
end

open("learning_rate.txt", "w") do io
end

"Sum from k=`from` to `to` of `a(k)`"
Î£(from::Integer, to::Integer, a::Function, zero = 0) = mapreduce(a, (+), from:to; init = zero)

"""Linear Regression
# Args:
    ğ°: Parameters
    Î¦(j, ğ±): Basis function of type (Int, Vector{T}) -> T
    ğ±: Input vector
"""
function y(ğ°::Vector{<:Number},
  Î¦::(T where T <: Function),
  ğ±::Vector{<:Number})::Number
    Î£(1, size(ğ°)[1], j->ğ°[j] * Î¦(j, ğ±))
end

"""Derivative of E_D with respect to ğ°â‚–
# Args:
    Î¦(k, ğ±â‚™): Basis function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    ğ°: Parameters
    k: Index for ğ°â‚– in respect to which the derivative is taken
    Ï‰: Weight decay factor
"""
function âˆ‚E_Dâˆ‚w_k(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number},
  k::Integer,
  Ï‰::Real)::Number
    N = size(t)[1]
    - Î£(1, N, n->Î¦(k, ğ—[n, :]) * (t[n] - y(ğ°, Î¦, ğ—[n,:])) - Ï‰*ğ°[k])
end

"""Error function
# Args:
    Î¦(k, ğ±â‚™): Basis function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    ğ°: Parameters
    Ï‰: Weight decay factor
"""
function E_D(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number},
  Ï‰::Real)::Number
    N = size(t)[1]
    1 // 2 * Î£(1, N, n->(t[n] - y(ğ°, Î¦, ğ—[n,:]))^2 + Ï‰*norm(ğ°)^2)
end

"""One iteration of the gradient descent algorithm
# Args:
    âˆ‚E_Dâˆ‚w_k: Partial derivative of error function with respect to
        the k-th parameter
    ğ—: Column vector of inputs ğ±â‚™ where ğ±â‚™ is an input vector to the
        error function
    t: corresponding target values for each ğ±â‚™
    ğ°: Parameters
    Î·: Learning rate
    âˆ‡ğ°_prior: Gradient of parameters from prior iteration
    Î³: Momentum factor
    Ï‰: Weight decay factor
"""
function gradient_descent_iteration(âˆ‚E_Dâˆ‚w_k::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  ğ°::Vector{<:Number},
  Î·::Number,
  âˆ‡ğ°_prior::Vector{<:Number},
  Î³::Number,
  Ï‰::Real)::Tuple{Vector{<:Number},Vector{<:Number}}
    M = size(ğ°)[1]
    âˆ‡ğ° = Î³ * âˆ‡ğ°_prior
    for j = 1:M
        âˆ‚E_Dâˆ‚w_jk(k) = âˆ‚E_Dâˆ‚w_k(ğ—, t, ğ°, k, Ï‰)
        âˆ‡ğ° += collect(map(âˆ‚E_Dâˆ‚w_jk, 1:M))

        open("norm_gradient_w.txt", "a") do io
            write(io, string(norm(âˆ‡ğ°)), "\n")
        end
    end
    (ğ° - Î· * âˆ‡ğ°, âˆ‡ğ°)
end

"""Minimize function E_D(ğ—, t, ğ°)
# Args:
    âˆ‚E_Dâˆ‚w_k: Partial derivative of error function with respect to
        the k-th parameter
    ğ—: Column vector of inputs ğ±â‚™ where ğ±â‚™ is an input vector to the
        error function
    t: corresponding target values for each ğ±â‚™
    ğ°: Initial parameters - usually `randn(M)`
    Î·: Learning rate
    M: Number of model parameters
    iters: Number of iterations
    Îµ: Gradient descent stops once the difference between two iterations
        (ğ° and ğ°') is less than Îµ
    Î³: Momentum factor
    Ï‰: Weight decay factor

# Fails:
    Fails on encountering NaN in computation or on Divergence to Inf
"""
function gradient_descent(âˆ‚E_Dâˆ‚w_k::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  Î·::Number,
  M::Integer,
  iters::Integer,
  ğ°::Vector{<:Number},
  Îµ = 10e-12::Number,
  Î³ = 0.9::Real,
  Ï‰ = 0.01::Real)::Tuple{Vector{<:Number},Integer}
    âˆ‡ğ° = zero(ğ°)
    did_iters = 0
    for i = 1:iters
        did_iters += 1
        ğ°_old = ğ°
        open("error.txt", "a") do io
            write(io, string(E_D(Î¦, ğ—, t, ğ°, Ï‰)), "\n")
        end
        open("learning_rate.txt", "a") do io
            write(io, string(Î·), "\n")
        end
        (ğ°, âˆ‡ğ°) = gradient_descent_iteration(âˆ‚E_Dâˆ‚w_k, ğ—, t, ğ°, Î·, âˆ‡ğ°, Î³, Ï‰)
         if any(isnan.(ğ°))
           error("Encountered NaN") 
        end
        if any(isinf.(ğ°))
            error("Divergence in calculation")
        end
        if norm(ğ°_old - ğ°) < Îµ
            break
        end
    end
    (ğ°, did_iters)
end

"""Find regression model 
# Args:
    Î¦: Basis Function
    ğ—: Set of inputs ğ±â‚™ where ğ±â‚™ is an input vector to Î¦
    t: corresponding target values for each ğ±â‚™
    Î·: learning rate with which to train
    M: Number of model parameters
    iters: Number of iterations
    Îµ: Gradient descent stops once the difference between
        two iterations (ğ° and ğ°') is less than Îµ
    Î³: Momentum Parameter
    Ï‰: Weight decay factor
    optimizer: Parameter to select optimizer that's used

# Fails:
    On unknown optimizers or error inside the optimizer
"""
function fit_linear_model(Î¦::Function,
  ğ—::Matrix{<:Number},
  t::Vector{<:Number},
  Î·::Number,
  M::Integer,
  iters::Integer,
  Îµ = 10e-12::Number,
  Î³ = 0.9::Real,
  Ï‰ = 0.01::Real,
  optimizer = :gradient_descent)::Tuple{Function,Number}
    if optimizer == :gradient_descent
        (ğ°, did_iters) = gradient_descent((ğ—, t, ğ°, k, Ï‰)->âˆ‚E_Dâˆ‚w_k(Î¦, ğ—, t, ğ°, k, Ï‰),
            ğ—, t, Î·, M, iters, randn(M), Îµ, Î³, Ï‰)
        residual_error = E_D(Î¦, ğ—, t, ğ°, Ï‰)
        println(ğ°, " after ", did_iters, " iterations. Residual error: ", residual_error)
        (ğ±->y(ğ°, Î¦, ğ±), residual_error)
    else
        error("Invalid optimizer")
    end
end

Î¦ = (j, ğ±) -> ğ±[1]^j

end